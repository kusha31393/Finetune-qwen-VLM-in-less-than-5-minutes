{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUCl12BabUaJc060Cr0Vnl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kusha31393/Finetune_qwen2.5_VL/blob/main/finetune_qwen_vl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen Vision Language Model Fine-tuning for LaTeX OCR\n",
        "\n",
        "This script fine-tunes the Qwen2-VL-7B-Instruct model to convert mathematical equation\n",
        "images into their corresponding LaTeX representations. The fine-tuning process uses\n",
        "LoRA (Low-Rank Adaptation) for efficient parameter-efficient training.\n",
        "\n",
        "Model: Qwen2-VL-7B-Instruct (4-bit quantized)\n",
        "Dataset: unsloth/Latex_OCR (mathematical equations with LaTeX labels)\n",
        "Training Method: Supervised Fine-Tuning (SFT) with LoRA adaptation\n",
        "\n",
        "Original Colab notebook located at:\n",
        "    https://colab.research.google.com/drive/1Ng4PP2AMkL69IApMyKt7QM2u-YF6sPHy"
      ],
      "metadata": {
        "id": "rApzdj3BG25y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-0hIiFnWuKm"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies for fine-tuning\n",
        "# - bitsandbytes: 4-bit quantization support\n",
        "# - accelerate: Multi-GPU training acceleration\n",
        "# - xformers: Memory-efficient attention implementation\n",
        "# - peft: Parameter-Efficient Fine-Tuning (LoRA)\n",
        "# - trl: Transformer Reinforcement Learning library\n",
        "# - unsloth: Fast training library for LLMs\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "hGvAgjm7Xi9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Available 4-bit quantized vision-language models\n",
        "# These models are optimized for efficient training with reduced memory usage\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Meta's Llama 3.2 Vision model\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\" # Alibaba's Qwen2-VL model\n",
        "]"
      ],
      "metadata": {
        "id": "GK4DIQYSYV11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained Qwen2-VL model with 4-bit quantization\n",
        "# This reduces memory usage from ~28GB to ~7GB while maintaining performance\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\", # Pre-quantized model from Unsloth\n",
        "    load_in_4bit=True,                       # Enable 4-bit quantization\n",
        "    use_gradient_checkpointing=\"unsloth\"     # Memory-efficient gradient computation\n",
        ")"
      ],
      "metadata": {
        "id": "I4gq8XxoZEiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
        "# This allows training only a small subset of parameters while maintaining performance\n",
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    # Component-specific fine-tuning flags\n",
        "    finetune_vision_layers=True,      # Train vision encoder layers\n",
        "    finetune_language_layers=True,    # Train language model layers\n",
        "    finetune_attention_modules=True,  # Train attention mechanisms\n",
        "    finetune_mlp_modules=True,        # Train feed-forward networks\n",
        "\n",
        "    # LoRA hyperparameters\n",
        "    r=16,                             # Rank of adaptation matrices (higher = more parameters)\n",
        "    lora_alpha=16,                    # LoRA scaling parameter (typically same as rank)\n",
        "    lora_dropout=0,                   # Dropout rate for LoRA layers (0 = no dropout)\n",
        "    bias=\"none\",                      # Bias configuration (\"none\", \"all\", or \"lora_only\")\n",
        "    random_state = 3407,              # Random seed for reproducibility\n",
        "    use_rslora=False,                 # Rank-Stabilized LoRA (experimental)\n",
        "    loftq_config=None                 # LoftQ quantization config (None = disabled)\n",
        ")"
      ],
      "metadata": {
        "id": "NEJoJwTHZx0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LaTeX OCR dataset containing mathematical equation images and their LaTeX code\n",
        "# This dataset contains thousands of mathematical expressions for training\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"unsloth/Latex_OCR\", split=\"train\")"
      ],
      "metadata": {
        "id": "4jwTPR23NcrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "LSwccvVvNqbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "WNQbjjTvNzue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]['image']"
      ],
      "metadata": {
        "id": "Ssvvj0WyN1tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[42837]['image']"
      ],
      "metadata": {
        "id": "h00pkEbgN5Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[42837]['text']"
      ],
      "metadata": {
        "id": "iKgGtJbNN9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the instruction prompt that will be used for all training samples\n",
        "# This tells the model what task to perform on the input image\n",
        "\n",
        "instruction = \"Write the LaTex representation for this image.\""
      ],
      "metadata": {
        "id": "1Rj7qdEyOSYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_conversation(sample):\n",
        "  \"\"\"\n",
        "    Convert a dataset sample into a conversation format for fine-tuning.\n",
        "\n",
        "    This function transforms raw image-text pairs into a structured conversation\n",
        "    format that the model expects during training. The conversation follows\n",
        "    the standard chat template with user and assistant roles.\n",
        "\n",
        "    Args:\n",
        "        sample (dict): Dataset sample containing 'image' and 'text' keys\n",
        "                      - 'image': PIL Image of mathematical equation\n",
        "                      - 'text': Corresponding LaTeX representation\n",
        "\n",
        "    Returns:\n",
        "        dict: Formatted conversation with 'messages' key containing:\n",
        "              - User message with instruction text and image\n",
        "              - Assistant message with LaTeX response\n",
        "  \"\"\"\n",
        "  conversation = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\"type\": \"text\", \"text\": instruction},        # Task instruction\n",
        "              {\"type\": \"image\", \"image\": sample[\"image\"]}   # Input image\n",
        "          ]\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\": [\n",
        "              {\"type\": \"text\", \"text\": sample[\"text\"]}      # Target LaTeX output\n",
        "          ]\n",
        "      }\n",
        "\n",
        "  ]\n",
        "  return {\"messages\": conversation}"
      ],
      "metadata": {
        "id": "7CADYD2YOfXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_conversation(dataset[0])"
      ],
      "metadata": {
        "id": "LIbcJ9AZPCUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the entire dataset to conversation format for training\n",
        "# This creates a list of formatted conversations for the SFTTrainer\n",
        "\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
      ],
      "metadata": {
        "id": "ysQWCMqMPGFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_dataset[1]"
      ],
      "metadata": {
        "id": "OVJYjs8FPN0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch model to inference mode for testing before training\n",
        "# This optimizes the model for generation rather than training\n",
        "\n",
        "FastVisionModel.for_inference(model)"
      ],
      "metadata": {
        "id": "bzd0hyPPPiJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pre-trained model's performance before fine-tuning\n",
        "# Use the second image from the dataset as a test case\n",
        "\n",
        "image = dataset[1][\"image\"]\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": instruction},  # Task instruction\n",
        "            {\"type\": \"image\", \"image\": image}       # Test image\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "5c0EP0qgRXF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare inputs for the model using the chat template\n",
        "# The tokenizer handles both text and image inputs simultaneously\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image, input_text,          # Process image and text together\n",
        "    add_special_tokens = False, # Special tokens already in chat template\n",
        "    return_tensors = \"pt\",      # Return PyTorch tensors\n",
        ").to(\"cuda\")                    # Move to GPU for inference"
      ],
      "metadata": {
        "id": "rQa47wbiSMlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate LaTeX output with streaming display\n",
        "# TextStreamer shows tokens as they're generated for real-time feedback\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True) # Don't repeat input\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer, # Stream output tokens\n",
        "    max_new_tokens=128,     # Maximum LaTeX length\n",
        "    use_cache=True,         # Use KV cache for efficiency\n",
        "    temperature=1.5,        # Sampling temperature (higher = more creative)\n",
        "    min_p=0.1               # Minimum probability threshold\n",
        ")"
      ],
      "metadata": {
        "id": "geJD2iYDSvSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "TgGfoacYTd04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import training components and prepare model for fine-tuning\n",
        "\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "-yAVs8XdTqlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch model back to training mode after inference testing\n",
        "\n",
        "FastVisionModel.for_inference(model)"
      ],
      "metadata": {
        "id": "eGNZrNk7Y9BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure and initialize the Supervised Fine-Tuning trainer\n",
        "# Uses specialized data collator for vision-language models\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Handles image-text pairs\n",
        "    train_dataset=converted_dataset,\n",
        "    args = SFTConfig(\n",
        "        # Batch size and gradient settings\n",
        "        per_device_train_batch_size=2,  # Small batch size for memory efficiency\n",
        "        gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
        "\n",
        "        # Training schedule\n",
        "        warmup_steps=5,   # Learning rate warmup\n",
        "        max_steps=30,     # Total training steps (quick demo)\n",
        "        learning_rate=2e-4,# Learning rate for LoRA parameters\n",
        "\n",
        "        # Precision settings (use bf16 if supported, else fp16)\n",
        "        fp16=not is_bf16_supported(),   # 16-bit floating point\n",
        "        bf16 = is_bf16_supported(),     # Brain float 16 (better than fp16)\n",
        "\n",
        "        # Logging and optimization\n",
        "        logging_steps=1,      # Log every step\n",
        "        optim=\"adamw_8bit\",   # 8-bit AdamW optimizer\n",
        "        weight_decay=0.01,    # L2 regularization\n",
        "        lr_scheduler_type=\"linear\", # Linear learning rate decay\n",
        "\n",
        "        # Reproducibility and output\n",
        "        seed=3407,               # Random seed\n",
        "        output_dir=\"outputs\",    # Save directory\n",
        "        report_to=\"none\",        # Disable wandb/tensorboard\n",
        "\n",
        "        # Dataset configuration\n",
        "        remove_unused_columns=False,                    # Keep all dataset columns\n",
        "        dataset_text_field=\"\",                          # No specific text field (using messages)\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},  # Use pre-formatted data\n",
        "        dataset_num_proc=4,                             # Parallel data processing\n",
        "        max_seq_length=2048,                            # Maximum sequence length\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "DQXs1y-MZCYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process\n",
        "# This will train the LoRA adapters on the LaTeX OCR task\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ke8kKn49cBqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch trained model back to inference mode for testing\n",
        "\n",
        "FastVisionModel.for_inference(model)"
      ],
      "metadata": {
        "id": "22vJWWIAcKk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = dataset[2][\"image\"]"
      ],
      "metadata": {
        "id": "Fm2Zhm9_chfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Write the LateX representation for this image.\""
      ],
      "metadata": {
        "id": "3NzfCOp1clBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test message for the fine-tuned model\n",
        "# Note: Image and text order is swapped compared to training format\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},                      # Image first\n",
        "        {\"type\": \"text\", \"text\": instruction}   # Then instruction\n",
        "    ]}\n",
        "]"
      ],
      "metadata": {
        "id": "liuY8RwjcqK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare inputs for fine-tuned model inference\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image,                      # Input image\n",
        "    input_text,                 # Formatted text prompt\n",
        "    add_special_tokens=False,   # Template already includes special tokens\n",
        "    return_tensors=\"pt\",        # PyTorch tensor format\n",
        ").to(\"cuda\")                    # Move to GPU"
      ],
      "metadata": {
        "id": "mr3ZZSzHc74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate LaTeX output from the fine-tuned model\n",
        "# Compare this output with the pre-training results to see improvement\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer, # Real-time output streaming\n",
        "    max_new_tokens=128,     # Maximum LaTeX code length\n",
        "    use_cache=True,         # Enable KV caching\n",
        "    temperature=1.5,        # Sampling temperature\n",
        "    min_p=0.1               # Minimum probability threshold\n",
        ")"
      ],
      "metadata": {
        "id": "FBoX4a07dPkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the test image to compare with fine-tuned model output\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "cDwjfXJWdnUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qaJYcR7Fdqdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}